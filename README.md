# LLM-inference

this is a repository for LLM inference with multiple GPUs
